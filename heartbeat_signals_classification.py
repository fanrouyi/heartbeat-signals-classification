from matplotlib import rcParams
import warnings
from sklearn.model_selection import train_test_split
rcParams['font.family'] = 'simhei'
rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, accuracy_score, auc, recall_score, f1_score, precision_score
import lightgbm as lgb
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
import joblib
import pandas as pd
import numpy as np
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.metrics import make_scorer
from sklearn.model_selection import cross_val_score
from bayes_opt import BayesianOptimization
import streamlit as st

# %%
# è¯»å–æ•°æ®
# df = pd.read_csv('train.csv')
# df1 = pd.read_csv('testA.csv')
# # %%
# print(df.head(5), df1.head(5))
# # %%
# data_list = []
# for item in df.values:
#     data_list.append([item[0]] + [float(i) for i in item[1].split(',')] + [item[2]])
# data = pd.DataFrame(np.array(data_list))
# data.columns = ['id'] + ['heartbeat' + str(i) for i in range(len(data_list[0]) - 2)] + ['label']
#joblib.dump(data,'data.csv')
data = joblib.load('data.csv')

# %%
def reduce_mem_usage(df):
    # å¤„ç†å‰ æ•°æ®é›†æ€»å†…å­˜è®¡ç®—
    start_mem = df.memory_usage().sum() / 1024 ** 2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))

    # éå†ç‰¹å¾åˆ—
    for col in df.columns:
        # å½“å‰ç‰¹å¾ç±»å‹
        col_type = df[col].dtype
        # å¤„ç† numeric å‹æ•°æ®
        if col_type != object:
            c_min = df[col].min()  # æœ€å°å€¼
            c_max = df[col].max()  # æœ€å¤§å€¼
            # int å‹æ•°æ® ç²¾åº¦è½¬æ¢
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
                    # float å‹æ•°æ® ç²¾åº¦è½¬æ¢
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        # å¤„ç† object å‹æ•°æ®
        else:
            df[col] = df[col].astype('category')  # object è½¬ category

    # å¤„ç†å æ•°æ®é›†æ€»å†…å­˜è®¡ç®—
    end_mem = df.memory_usage().sum() / 1024 ** 2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))

    return df


# %%
data = reduce_mem_usage(data)
# %%
print(data.head(5), data.describe(), data.info())
# %%
print(data.isnull().sum())
#%%
import seaborn as sns
sns.boxplot((data.drop(['id','label'],axis= 1)))
# %%
# æŸ¥çœ‹é‡å¤å€¼
duplicates = data[data.duplicated()]
print(duplicates)
# %%
fig = plt.figure()
plt.hist(data['label'], orientation='vertical', histtype='bar')
plt.show()
# %%
x = data.drop(['id', 'label'], axis=1)
y = data['label']
print(x.shape, y.shape)
# %%
target_count = np.bincount(y)
somte = SMOTE(sampling_strategy={1: target_count[3]}, random_state=42)
x_resample, y_resample = somte.fit_resample(x, y)
# %%
print(np.bincount(y_resample))
# %%
x_train, x_test, y_train, y_test = train_test_split(x_resample, y_resample, train_size=0.8, random_state=10)
# %%
# model_list = [MultinomialNB(),
#               DecisionTreeClassifier(criterion='entropy', max_depth=5, class_weight='balanced'),
#               SVC(probability=True, kernel='rbf', class_weight='balanced')]
# model_names = ['Naive_Bayes', 'Decision_Tree', 'SVC']
#
# for name, model in zip(model_names, model_list):
#     # model.fit(x_train, y_train)
#     model = joblib.load(f'{name}.pkl')
#     y_predict = model.predict(x_test)
#     # joblib.dump(model,'{}.pkl'.format(name))
#     print('{}åˆ†ç±»æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¯„ä»·ç»“æœä¸ºï¼š'.format(name))
#     print(classification_report(y_test, y_predict))


# %%
def f1_score_vali(preds, data_vali):
    labels = data_vali.get_label()
    preds = np.argmax(preds.reshape(4, -1), axis=0)
    score_vali = f1_score(y_true=labels, y_pred=preds, average='macro')
    return 'f1_score', score_vali, True


class_weights = compute_sample_weight('balanced', y_train)
train_matrix = lgb.Dataset(x_train, label=y_train, weight=class_weights)
test_matrix = lgb.Dataset(x_test, label=y_test)
# params = {
#     "learning_rate": 0.1,
#     "boosting": 'gbdt',
#     "lambda_l2": 0.1,
#     "max_depth": -1,
#     "num_leaves": 128,
#     "bagging_fraction": 0.8,
#     "feature_fraction": 0.8,
#     "metric": None,
#     "objective": "multiclass",
#     "num_class": 4,
#     "nthread": 10,
#     "verbose": -1,
# }

# """ä½¿ç”¨è®­ç»ƒé›†æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒ"""
# model = lgb.train(params,
#                   train_set=train_matrix,
#                   valid_sets=test_matrix,
#                   num_boost_round=2000,
#                   feval=f1_score_vali
#                  )

# y_pred = model.predict(x_test,num_iteration=model.best_iteration)
# y_pred = np.argmax(y_pred, axis=1)
# print(classification_report(y_pred,y_test))
# %%
# """å®šä¹‰ä¼˜åŒ–å‡½æ•°"""
# def rf_cv_lgb(num_leaves, max_depth, bagging_fraction, feature_fraction, bagging_freq, min_data_in_leaf,
#               min_child_weight, min_split_gain, reg_lambda, reg_alpha):
#     # å»ºç«‹æ¨¡å‹
#     model_lgb = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', num_class=4,
#                                    learning_rate=0.1, n_estimators=5000,
#                                    num_leaves=int(num_leaves), max_depth=int(max_depth),
#                                    bagging_fraction=round(bagging_fraction, 2), feature_fraction=round(feature_fraction, 2),
#                                    bagging_freq=int(bagging_freq), min_data_in_leaf=int(min_data_in_leaf),
#                                    min_child_weight=min_child_weight, min_split_gain=min_split_gain,
#                                    reg_lambda=reg_lambda, reg_alpha=reg_alpha,
#                                    n_jobs= 8
#                                   )
#     f1 = make_scorer(f1_score, average='micro')
#     val = cross_val_score(model_lgb, x_train, y_train, cv=5, scoring=f1).mean()

#     return val
# #"""å®šä¹‰ä¼˜åŒ–å‚æ•°"""
# bayes_lgb = BayesianOptimization(
#     rf_cv_lgb,
#     {
#         'num_leaves':(10, 200),
#         'max_depth':(3, 20),
#         'bagging_fraction':(0.5, 1.0),
#         'feature_fraction':(0.5, 1.0),
#         'bagging_freq':(0, 100),
#         'min_data_in_leaf':(10,100),
#         'min_child_weight':(0, 10),
#         'min_split_gain':(0.0, 1.0),
#         'reg_alpha':(0.0, 10),
#         'reg_lambda':(0.0, 10),
#     }
# )

# #"""å¼€å§‹ä¼˜åŒ–"""
# bayes_lgb.maximize(n_iter=10)

# params = bayes_lgb.max
# print(params)
# joblib.dump(params,'params.pkl')
# %%
base_params_lgb = {
    'boosting_type': 'gbdt',
    'objective': 'multiclass',
    'num_class': 4,
    'learning_rate': 0.01,
    'num_leaves': 84,
    'max_depth': 19,
    'min_data_in_leaf': 78,
    'min_child_weight': 7.7,
    'bagging_fraction': 0.57,
    'feature_fraction': 0.63,
    'bagging_freq': 61,
    'reg_lambda': 0.4,
    'reg_alpha': 1.13,
    'min_split_gain': 0.05,
    'nthread': 10,
    'verbose': -1,
}

# cv_result_lgb = lgb.cv(
#     train_set=train_matrix,
#     num_boost_round=20000,
#     nfold=5,
#     stratified=True,
#     shuffle=True,
#     params=base_params_lgb,
#     feval=f1_score_vali,
#     seed=0
# )
# print('è¿­ä»£æ¬¡æ•°{}'.format(len(cv_result_lgb['f1_score-mean'])))
# print('æœ€ç»ˆæ¨¡å‹çš„f1ä¸º{}'.format(max(cv_result_lgb['f1_score-mean'])))
# %%
model = joblib.load('LightGBM.pkl')
# model = lgb.train(base_params_lgb, train_set=train_matrix, num_boost_round=4833,
#                   feval=f1_score_vali)
y_pred_prob = model.predict(x_test, num_iteration=model.best_iteration)
y_pred = np.argmax(y_pred_prob, axis=1)
# joblib.dump(model,'LightGBM.pkl')
# %%
classification_report = classification_report(y_pred, y_test,output_dict=True)
from sklearn.metrics import auc, roc_curve, roc_auc_score, confusion_matrix
from sklearn.preprocessing import LabelBinarizer

# %%
st.sidebar.image("logo.png")
st.sidebar.title("åŸºäºLightGBMçš„å¿ƒè·³ä¿¡å·åˆ†ç±»é¢„æµ‹")
st.sidebar.write('Made By FanRou yi')
st.sidebar.markdown('------')
st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Fredoka+One&display=swap');

        .artistic-title {
            font-family: 'Fredoka One';
            font-size: 70px;

            color: #FE4676;
            text-align: center;
            text-shadow: 3px 3px 8px rgba(0, 0, 0, 0.4);
            letter-spacing: 2px;
            margin-top: 30px;
        }
    </style>
    <div class="artistic-title">
        Haertbeat Signals 
            Classification
    </div>
""", unsafe_allow_html=True)
with st.expander("ğŸ“–æœ‰å…³è¿™ä¸ªstreamlit appçš„è¯¦ç»†ä»‹ç»"):
    st.write("ä½ å¥½ï¼æ¬¢è¿æ¥åˆ°Heartbeat Signals Classificationã€‚ğŸ¤š\n\n"
             "è¿™æ˜¯ä¸€ä¸ªåŸºäºLightGBMçš„å¿ƒè·³ä¿¡å·åˆ†ç±»é¢„æµ‹æ¨¡å‹çš„streamlit appï¼Œappä¸­çš„åŠŸèƒ½åŒ…æ‹¬ï¼šè¿›è¡Œæ ·æœ¬é¢„æµ‹ï¼ŒæŸ¥çœ‹æ¨¡å‹æ€§èƒ½ï¼ŒæŸ¥çœ‹æ¨¡å‹è®­ç»ƒæºç ã€‚\n\n"
             "ä½ å¯ä»¥è¾“å…¥ä¸€ä¸ªæ ·æœ¬çš„æ ·æœ¬ç‰¹å¾æ¥è¿›è¡Œé¢„æµ‹ï¼Œä½†è¯·è¾“å…¥ä»¥é€—å·ä¸ºé—´éš”çš„205ä¸ªç‰¹å¾ï¼Œè¿™æ ·æ‰å¯ä»¥é¡ºåˆ©è¿›è¡Œé¢„æµ‹ï¼Œæˆ‘ä»¬ä¼šè¾“å‡ºç»™ä½ é¢„æµ‹ç»“æœã€‚\n\n"
             "ä½ ä¹Ÿå¯ä»¥æŸ¥çœ‹æœ¬ç³»ç»Ÿæ‰€ä½¿ç”¨çš„æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼Œæˆ‘ä»¬ä¼šæ˜¾ç¤ºæ¨¡å‹çš„åˆ†ç±»æŠ¥å‘Šï¼Œå…¶ä¸­åŒ…å«å¯¹æ¯ä¸ªæ ‡ç­¾é¢„æµ‹çš„æŸ¥å‡†ç‡ï¼ŒæŸ¥å…¨ç‡ï¼Œf1ç­‰ç­‰ï¼Œè¿˜æœ‰æ¯ä¸ªæ ‡ç­¾çš„ROCå›¾ã€‚\n\n"
             "æ­¤å¤–ï¼Œä½ è¿˜å¯ä»¥è·å–æœ¬é¡¹ç›®çš„æºç ï¼Œé‡Œé¢åŒ…å«æœ‰å¯¹æ¨¡å‹çš„é€‰æ‹©ï¼Œè®­ç»ƒï¼Œä»¥åŠéƒ¨ç½²è¿‡ç¨‹ï¼Œè¿˜æœ‰æ¨¡å‹è®­ç»ƒæ‰€ä½¿ç”¨çš„æ•°æ®æ¥æºã€‚")
st.markdown("""
    <style>
        /* å®šä¹‰ä¾§è¾¹æ ç›®å½•é¡¹çš„æ ·å¼ */
        .sidebar-link {
            display: block;
            padding: 12px;
            margin: 8px 0;
            border: 1.7px solid #FE4676;  /* è®¾ç½®è¾¹æ¡†é¢œè‰² */
            border-radius: 8px;         /* è®¾ç½®åœ†è§’ */
            background-color: #f1f1f1;  /* è®¾ç½®èƒŒæ™¯è‰² */
            text-decoration: none;      /* å»é™¤é“¾æ¥ä¸‹åˆ’çº¿ */
            text-align: center;
            color:#4E4E4E ;             /* è®¾ç½®æ–‡å­—é¢œè‰² */
            font-weight: bold;          /* è®¾ç½®å­—ä½“åŠ ç²— */
        }


        /* ç»™å†…å®¹éƒ¨åˆ†å¢åŠ ä¸€ç‚¹é—´è· */
        .content-section {
            margin-top: 40px;
        }
    </style>
""", unsafe_allow_html=True)

st.sidebar.markdown('<a href="#section-1" class="sidebar-link">ğŸ’»æ ·æœ¬é¢„æµ‹</a>', unsafe_allow_html=True)
st.sidebar.markdown('<a href="#section-2" class="sidebar-link">ğŸ”æ¨¡å‹æ€§èƒ½</a>', unsafe_allow_html=True)
st.sidebar.markdown('<a href="#section-3" class="sidebar-link">ğŸššä»£ç å’Œæ•°æ®æ¥æº</a>', unsafe_allow_html=True)

# ä¸»æ å†…å®¹åŒºåŸŸ
st.markdown('<div id="section-1"></div>', unsafe_allow_html=True)
st.markdown("## ğŸ’»æ ·æœ¬é¢„æµ‹")
model = joblib.load('LightGBM.pkl')
user_input = st.text_input("è¯·è¾“å…¥ä¸€ç»„ä»¥é€—å·ä¸ºé—´éš”çš„æ ·æœ¬ç‰¹å¾", "")
if st.button("ğŸ¯ é¢„æµ‹"):
    if user_input:
        try:
            input_features = np.array([float(x) for x in user_input.split(',')])
            input_x = pd.DataFrame(input_features.reshape(-1, 1))
            prediction_prob = model.predict(input_x[0], predict_disable_shape_check=True)
            prediction = np.argmax(prediction_prob, axis=1)
            st.write(prediction)
        except ValueError:
            st.error("è¾“å…¥æ— æ•ˆï¼Œè¯·ç¡®ä¿æ¯ä¸ªç‰¹å¾å€¼æ˜¯æ•°å­—å¹¶ä»¥é€—å·åˆ†éš”ã€‚")
    else:
        st.error("è¯·è¾“å…¥ç‰¹å¾å€¼è¿›è¡Œé¢„æµ‹ã€‚")

st.markdown('<div id="section-2"></div>', unsafe_allow_html=True)
st.markdown("## ğŸ”æ¨¡å‹æ€§èƒ½")
st.subheader("ğŸ’Œ Classification Report")
report_matrix = pd.DataFrame(classification_report).transpose()
st.dataframe(report_matrix)
st.subheader('ğŸ“ˆ æ¯ä¸ªç±»åˆ«çš„ROCå›¾')
lb = LabelBinarizer()
y_test_bin = lb.fit_transform(y_test)
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(4):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
colors = ['blue', 'red', 'green', 'black']
with st.container():
    col1,col2 = st.columns(2)
    col3, col4 = st.columns(2)
    cols = [col1, col2, col3, col4]
    for i in range(4):
        with cols[i]:
            ax = plt.figure()
            plt.plot(fpr[i], tpr[i], color=colors[i], lw=2, label='ROC curve of class {0} (area = {1:0.2f})'
                                                                  ''.format(i, roc_auc[i]))
            plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')
            plt.legend(loc='lower right')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'label{i} ROC Curve')
            plt.show()
            st.pyplot(ax)

st.markdown('<div id="section-3"></div>', unsafe_allow_html=True)
st.markdown("## ğŸššä»£ç å’Œæ•°æ®æ¥æº")
with st.expander("ğŸ“‹ç¨‹åºæºç "):
    st.markdown("[â˜ï¸æœ¬ç¨‹åºçš„ä»£ç ï¼Œå†…å®¹åŒ…å«æ•°æ®å¤„ç†ï¼Œæ¨¡å‹é€‰æ‹©ï¼Œæ¨¡å‹è®­ç»ƒå’Œå‰ç«¯éƒ¨ç½²](https://github.com/fanrouyi/heartbeat-signals-classification.git)")
with st.expander("ğŸ“‚æ•°æ®æ¥æº"):
    st.markdown(
        "[â˜ï¸æ¨¡å‹è®­ç»ƒä½¿ç”¨çš„æ•°æ®æ¥æºä¸ºå¤©æ± æ•°æ®é›†ä¸­çš„å¿ƒè·³ä¿¡å·æ•°æ®é›†ï¼Œæœ¬ç¨‹åºçš„æ¨¡å‹ä½¿ç”¨äº†å…¶ä¸­åä¸‡æ¡æ ·æœ¬](https://tianchi.aliyun.com/dataset/167192)")

